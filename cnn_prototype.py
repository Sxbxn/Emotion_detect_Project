# -*- coding: utf-8 -*-
"""resnet_transferlearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17ag69BjGrH4NM4_DF4MAVM1oQ9wnkSlw

# CNN 설계
"""

import torch
import torch.nn as nn
import torchvision
import torch.optim as optim
import torch.nn.functional as F
from torchvision import transforms, datasets, models
import os
from matplotlib import pyplot as plt
from PIL import Image
import numpy as np

USE_CUDA = torch.cuda.is_available()
DEVICE = torch.device("cuda" if USE_CUDA else "cpu")
print(DEVICE)

# !pip install kaggle --upgrade

# os.environ['KAGGLE_USERNAME'] = 'jungyuchoi'
# os.environ['KAGGLE_KEY'] = '9431599f2458bdba977477e62d1dd272'
# !kaggle datasets download -d ananthu017/emotion-detection-fer
# !unzip '*.zip'

!ls
torch.cuda.empty_cache()
import gc

gc.collect()

"""## 하이퍼파라미터 """

EPOCHS     = 20
BATCH_SIZE = 128

"""## 데이터셋 불러오기"""

trans = transforms.Compose([
                            transforms.ToTensor(),
                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

train_data = torchvision.datasets.ImageFolder(root='./train', transform=trans)
test_data = torchvision.datasets.ImageFolder(root='./test', transform=trans)

train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = BATCH_SIZE, shuffle = True)
test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = BATCH_SIZE, shuffle = True)

classes = ('angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised')

dataiter = iter(train_loader)
images, labels = dataiter.next()
print(images.shape)
img = Image.open('./train/angry/im1.png')
plt.imshow(np.asarray(img))
print(labels)

"""## 모델


"""

# 미리 훈련된 ResNet 모델을 다운로드하고 불러옴
# torchvision.models 참고

import torchvision.models as models

# model = models.resnet50(pretrained=True)
# print(model) # 불러온 모델 구조 확인

class Model(nn.Module):
    def __init__(self):
      super(Model, self).__init__()
      self.conv1 = nn.Conv2d(3, 64, 3)
      self.conv2 = nn.Conv2d(64, 64, 3)
      self.conv3 = nn.Conv2d(64, 128, 3)
      self.conv4 = nn.Conv2d(128, 128, 3)
      self.conv5 = nn.Conv2d(128, 512, 3)
      self.conv6 = nn.Conv2d(512, 512, 3)
      self.bn_64 = nn.BatchNorm2d(64)
      self.bn_128 = nn.BatchNorm2d(128)
      self.bn_256 = nn.BatchNorm2d(512)
      self.conv_drop_out = nn.Dropout2d(p=0.15)
      self.fc1 = nn.Linear(2048, 128)
      self.fc2 = nn.Linear(128, 128)
      self.fc3 = nn.Linear(128, 7)
      self.bn_1d = nn.BatchNorm1d(128)
      self.fc_drop_out = nn.Dropout2d(p=0.3)

    def forward(self, x):
      x = F.relu(self.bn_64(self.conv1(x)))
      x = F.relu(self.conv_drop_out(F.max_pool2d(self.bn_64(self.conv2(x)), 2)))
      x = F.relu(self.bn_128(self.conv3(x)))
      x = F.relu(self.conv_drop_out(F.max_pool2d(self.bn_128(self.conv4(x)), 2)))
      x = F.relu(self.bn_256(self.conv5(x)))
      x = F.relu(self.conv_drop_out(F.max_pool2d(self.bn_256(self.conv6(x)), 2)))
      x = x.view(-1, 2048)
      x = F.relu(self.fc_drop_out(self.fc1(x)))
      x = F.relu(self.fc_drop_out(self.fc2(x)))
      x = self.fc3(x)
      return x

model = Model()

# 제일 마지막 FC layer를 Cifar 클래스에 맞도록 Output 크기 교체 == 즉, resnet.fc를 새로운 nn.Linear를 정의하여 할당.
# 기존 ResNet 모델 FC layer의 in_features값을 할당
#num_input을 nn.Linear에 넣고 수정

# model.fc = nn.Linear(model.fc.in_features, 7)

"""## 준비"""

model.to(DEVICE)

optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)#0.1 vs 0.001
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

"""## 학습하기"""

def train(model, train_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(DEVICE), target.to(DEVICE)
        optimizer.zero_grad()
        output = model(data)
        loss = F.cross_entropy(output, target)
        loss.backward()
        optimizer.step()

"""## 테스트하기"""

def evaluate(model, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(DEVICE), target.to(DEVICE)
            output = model(data)

            # 배치 오차를 합산
            test_loss += F.cross_entropy(output, target,
                                         reduction='sum').item()

            # 가장 높은 값을 가진 인덱스가 바로 예측값
            pred = output.max(1, keepdim=True)[1]
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)
    test_accuracy = 100. * correct / len(test_loader.dataset)
    return test_loss, test_accuracy

"""## 학습"""

loss = []
accuracy = []
for epoch in range(1, EPOCHS + 1):
    train(model, train_loader, optimizer, epoch)
    scheduler.step()
    test_loss, test_accuracy = evaluate(model, test_loader)
    
    loss.append(test_loss)
    accuracy.append(test_accuracy)
    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(
          epoch, test_loss, test_accuracy))
plt.plot(loss)
plt.plot(accuracy)
plt.ylim(0, 100)
plt.xlabel('Epoch')
plt.ylabel('Accuracy')