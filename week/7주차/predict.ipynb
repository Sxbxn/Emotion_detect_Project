{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VVOunCYdsfx"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets, models\n",
        "from torchvision.utils import make_grid\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il9soj17d9vT",
        "outputId": "e875575c-cf6f-4747-c50e-99678a457e61"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "print(DEVICE)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mP2O8Tuh6ey"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Model, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(1, 16, 3)\n",
        "      self.conv2 = nn.Conv2d(16, 32, 3)\n",
        "      self.conv3 = nn.Conv2d(32, 64, 3)\n",
        "      self.conv4 = nn.Conv2d(64, 32, 3)\n",
        "      self.fc1 = nn.Linear(2048, 1024)\n",
        "      self.fc2 = nn.Linear(1024, 128)\n",
        "      self.fc3 = nn.Linear(128, 7)\n",
        "      self.batch_norm_16 = nn.BatchNorm2d(16)\n",
        "      self.batch_norm_32 = nn.BatchNorm2d(32)\n",
        "      self.batch_norm_64 = nn.BatchNorm2d(64)\n",
        "      self.drop_out = nn.Dropout2d(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "      x = F.relu(self.batch_norm_16(self.conv1(x)))\n",
        "      x = F.relu(self.drop_out(F.max_pool2d(self.batch_norm_32(self.conv2(x)), 2)))\n",
        "      x = F.relu(self.drop_out(F.max_pool2d(self.batch_norm_64(self.conv3(x)), 2)))\n",
        "      x = F.relu(self.drop_out(self.batch_norm_32(self.conv4(x))))\n",
        "      x = x.view(-1, 2048)\n",
        "\n",
        "      x = F.relu(self.drop_out(self.fc1(x)))\n",
        "      x = F.relu(self.drop_out(self.fc2(x)))\n",
        "      x = self.fc3(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "model = Model()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekwvf692h7VT",
        "outputId": "1c278aef-5e59-48a6-e815-8cfc14ca6b36"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/model/model_rotate.pt'))\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's state_dict:\n",
            "conv1.weight \t torch.Size([16, 1, 3, 3])\n",
            "conv1.bias \t torch.Size([16])\n",
            "conv2.weight \t torch.Size([32, 16, 3, 3])\n",
            "conv2.bias \t torch.Size([32])\n",
            "conv3.weight \t torch.Size([64, 32, 3, 3])\n",
            "conv3.bias \t torch.Size([64])\n",
            "conv4.weight \t torch.Size([32, 64, 3, 3])\n",
            "conv4.bias \t torch.Size([32])\n",
            "fc1.weight \t torch.Size([1024, 2048])\n",
            "fc1.bias \t torch.Size([1024])\n",
            "fc2.weight \t torch.Size([128, 1024])\n",
            "fc2.bias \t torch.Size([128])\n",
            "fc3.weight \t torch.Size([7, 128])\n",
            "fc3.bias \t torch.Size([7])\n",
            "batch_norm_16.weight \t torch.Size([16])\n",
            "batch_norm_16.bias \t torch.Size([16])\n",
            "batch_norm_16.running_mean \t torch.Size([16])\n",
            "batch_norm_16.running_var \t torch.Size([16])\n",
            "batch_norm_16.num_batches_tracked \t torch.Size([])\n",
            "batch_norm_32.weight \t torch.Size([32])\n",
            "batch_norm_32.bias \t torch.Size([32])\n",
            "batch_norm_32.running_mean \t torch.Size([32])\n",
            "batch_norm_32.running_var \t torch.Size([32])\n",
            "batch_norm_32.num_batches_tracked \t torch.Size([])\n",
            "batch_norm_64.weight \t torch.Size([64])\n",
            "batch_norm_64.bias \t torch.Size([64])\n",
            "batch_norm_64.running_mean \t torch.Size([64])\n",
            "batch_norm_64.running_var \t torch.Size([64])\n",
            "batch_norm_64.num_batches_tracked \t torch.Size([])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHNXpE99ii76",
        "outputId": "913646be-e728-4aa7-b29c-b9eae0e080b0"
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "  (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=7, bias=True)\n",
              "  (batch_norm_16): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batch_norm_32): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batch_norm_64): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (drop_out): Dropout2d(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTdpIHqnkIUU"
      },
      "source": [
        "import cv2, requests\n",
        "\n",
        "facecasc = cv2.CascadeClassifier(\"/content/drive/MyDrive/model/haarcascade_frontalface_default.xml\")\n",
        "\n",
        "def emotion_detect(url):\n",
        "    # frame = cv2.imread('/content/drive/MyDrive/Colab Notebooks/facial_emotion_recog/ì˜¤.png')\n",
        "    raw = requests.get(url).content\n",
        "    nparr = np.asarray(bytearray(raw), dtype=np.uint8)\n",
        "    frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "    \n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = facecasc.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
        "    \n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(frame, (x, y - 50), (x + w, y + h + 10), (255, 0, 0), 2)\n",
        "        roi_gray = gray[y : y + h, x : x + w]\n",
        "        cropped_img = np.expand_dims(\n",
        "            np.expand_dims(cv2.resize(roi_gray, (16, 16)), -1), 0\n",
        "        )\n",
        "        cropped_img = torch.from_numpy(cropped_img)\n",
        "        prediction = model(cropped_img)\n",
        "\n",
        "        p = prediction.tolist()\n",
        "        print(p)\n",
        "\n",
        "        \n",
        "        maxindex = int(np.argmax(prediction))\n",
        "        \n",
        "        print(\"DEBUG: {}\".format(maxindex))\n",
        "        text_img = Image.fromarray(frame)\n",
        "        draw = ImageDraw.Draw(text_img)\n",
        "        \n",
        "        draw.text((x+5, y-30), emotion_dict[maxindex], font=font, fill=(255, 255, 255))\n",
        "        \n",
        "        frame = np.array(text_img)\n",
        "        # cv2.putText(\n",
        "        #     frame,\n",
        "        #     emotion_dict[maxindex],\n",
        "        #     (x + 20, y - 5),\n",
        "        #     cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        #     1,\n",
        "        #     (255, 255, 255),\n",
        "        #     2,\n",
        "        #     cv2.LINE_AA,\n",
        "        # )\n",
        "    try:\n",
        "        print(\"result: {}\".format(emotion_dict[maxindex]))\n",
        "    except Exception as err:\n",
        "        print(err)\n",
        "        return\n",
        "\n",
        "    return frame"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vXG6rdIkWAg",
        "outputId": "a0ba348e-79d0-41a0-9777-f2cbed20f4ae"
      },
      "source": [
        "url = input()\n",
        "# out =requests.get(url).content\n",
        "# print(out)\n",
        "# emotion_detect(url)\n",
        "\n",
        "raw = requests.get(url).content\n",
        "nparr = np.asarray(bytearray(raw), dtype=np.uint8)\n",
        "nparr = cv2.resize(nparr, (48, 48), 0)\n",
        "\n",
        "nparr = nparr[np.newaxis, np.newaxis,:,:]\n",
        "\n",
        "nparr = torch.from_numpy(nparr).float()\n",
        "\n",
        "out = model(nparr) # predict\n",
        "p = out.tolist()\n",
        "print(p)\n",
        "lis = out.flatten().tolist()\n",
        "lis = list(map(lambda x: x+100, lis))\n",
        "result = [int(round(a / sum(lis), 3)*100) for a in lis]\n",
        "classes = ['neutral', 'fearful', 'sad', 'happy', 'surprised', 'angry', 'disgusted']\n",
        "\n",
        "print(\"í‘œì • :\", classes)\n",
        "print(\"í™•ë¥  :\",result)\n",
        "\n",
        "maxindex = int(np.argmax(p))\n",
        "\n",
        "print(\"ê²°ê³¼ :\",classes[maxindex])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://imgnn.seoul.co.kr/img/upload/2015/05/07/SSI_20150507184218_V.jpg\n",
            "[[-19.769227981567383, -88.49606323242188, 0.3338146507740021, 19.73162078857422, -1.4216477870941162, 16.407264709472656, -9.704778671264648]]\n",
            "í‘œì • : ['neutral', 'fearful', 'sad', 'happy', 'surprised', 'angry', 'disgusted']\n",
            "í™•ë¥  : [13, 1, 16, 19, 16, 18, 14]\n",
            "ê²°ê³¼ : happy\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}